# -*- coding: utf-8 -*-
"""Rain_in_australia_randomTrees.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17wm4b9g662CHJ_CktAieSbRENuoeDcsQ
"""

!pip install opendatasets --quiet

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib
import opendatasets as od
import plotly.express as px

matplotlib.rcParams['font.size'] = 14
matplotlib.rcParams['figure.figsize'] = (9, 5)
matplotlib.rcParams['figure.facecolor'] = '#00000000'

od.download('https://www.kaggle.com/jsphyg/weather-dataset-rattle-package')

df= pd.read_csv('/content/weather-dataset-rattle-package/weatherAUS.csv')

df

df.info()

df.nunique()

df.dropna(subset=['RainTomorrow'],inplace=True)

sns.set_style('darkgrid')
matplotlib.rcParams['figure.figsize']=(10,6)
matplotlib.rcParams['font.size']=14

px.histogram(df,x='Location',color='RainToday')

df.Location.nunique()

px.histogram(df,x='Temp3pm',title='Temeprature at 3 pm vs RainTomorrow',color='RainTomorrow')

px.histogram(df,x='RainTomorrow',color='RainToday',title='Rain Tomorrow vs Rain Today')

px.scatter(df,title='minimum temp vs maximum temp',x='MinTemp',y='MaxTemp',color='RainToday')

px.scatter(df.sample(2000),title='minimum temp vs maximum temp',x='MinTemp',y='MaxTemp',color='RainToday')

px.strip(df.sample(2000),title='Temp(3pm) vs Humidity3pm',x='Temp3pm',y='Humidity3pm',color='RainTomorrow')

px.strip(df.sample(2000),title='Temp(9am) vs Humidity9am',x='Temp9am',y='Humidity9am',color='RainToday')

df['year'] = pd.to_datetime(df['Date']).dt.year

df['year'] = pd.to_datetime(df['Date']).dt.year.astype(int) # Convert 'year' column to integers
train_df = df[df['year'] < 2015]
val_df = df[df['year'] == 2015]
test_df = df[df['year'] > 2015]

train_df

input_cols=list(df.columns)[2:-2]
target_cols='RainTomorrow'

train_inputs=train_df[input_cols].copy()
train_targets=train_df[target_cols].copy()
val_inputs=val_df[input_cols].copy()
val_targets=val_df[target_cols].copy()
test_inputs=test_df[input_cols].copy()
test_targets=test_df[target_cols].copy()

train_inputs

numeric_cols=train_inputs.select_dtypes(include=np.number).columns.tolist()
categorical_cols=train_inputs.select_dtypes('object').columns.tolist()

numeric_cols

categorical_cols

train_inputs.isna().sum()

from sklearn.impute import SimpleImputer

?SimpleImputer

imputer=SimpleImputer(strategy='mean')

train_inputs = train_df[input_cols].copy()
val_inputs = val_df[input_cols].copy()
test_inputs = test_df[input_cols].copy()
imputer=imputer.fit(df[numeric_cols])

train_inputs[numeric_cols]=imputer.transform(train_inputs[numeric_cols])
val_inputs[numeric_cols]=imputer.transform(val_inputs[numeric_cols])
test_inputs[numeric_cols]=imputer.transform(test_inputs[numeric_cols])

df.describe().loc[['min','max']]

from sklearn.preprocessing import MinMaxScaler

scaler=MinMaxScaler().fit(df[numeric_cols])

train_inputs[numeric_cols]=scaler.transform(train_inputs[numeric_cols])
val_inputs[numeric_cols]=scaler.transform(val_inputs[numeric_cols])
test_inputs[numeric_cols]=scaler.transform(test_inputs[numeric_cols])

train_inputs

from sklearn.preprocessing import OneHotEncoder

train_inputs=train_inputs[categorical_cols].fillna('Unknown')
val_inputs=val_inputs[categorical_cols].fillna('Unknown')
test_inputs=test_inputs[categorical_cols].fillna('Unknown')

encoder = OneHotEncoder(handle_unknown='ignore').fit(train_inputs[categorical_cols])

encoded_cols=list(encoder.get_feature_names_out(categorical_cols))

encoded_cols

# Convert the sparse matrix to a dense array and create a DataFrame
train_encoded = pd.DataFrame(encoder.transform(train_inputs[categorical_cols]).toarray(), columns=encoded_cols, index=train_inputs.index)
val_encoded = pd.DataFrame(encoder.transform(val_inputs[categorical_cols]).toarray(), columns=encoded_cols, index=val_inputs.index)
test_encoded = pd.DataFrame(encoder.transform(test_inputs[categorical_cols]).toarray(), columns=encoded_cols, index=test_inputs.index)

# Create copies of the original DataFrames before filtering to preserve numeric columns
train_inputs_original = train_df[input_cols].copy()
val_inputs_original = val_df[input_cols].copy()
test_inputs_original = test_df[input_cols].copy()

# Concatenate the encoded features with the original DataFrame containing numeric features
train_numeric = pd.DataFrame(train_inputs_original[numeric_cols], columns=numeric_cols, index=train_inputs_original.index)
val_numeric = pd.DataFrame(val_inputs_original[numeric_cols], columns=numeric_cols, index=val_inputs_original.index)
test_numeric = pd.DataFrame(test_inputs_original[numeric_cols], columns=numeric_cols, index=test_inputs_original.index)


X_train = pd.concat([train_numeric, train_encoded], axis=1)
X_val = pd.concat([val_numeric, val_encoded], axis=1)
X_test = pd.concat([test_numeric, test_encoded], axis=1)

X_train

from sklearn.tree import DecisionTreeClassifier

model= DecisionTreeClassifier(random_state=42)

model.fit(X_train,train_targets)

from sklearn.metrics import accuracy_score , confusion_matrix

train_pred= model.predict(X_train)

train_pred

pd.value_counts(train_pred)

pd.value_counts(train_targets)

accuracy_score(train_pred,train_targets)

train_prob=model.predict_proba(X_train)
train_prob

val_pred= model.predict(X_val)

accuracy_score(val_pred,val_targets)

val_targets

val_pred

model.score(X_val,val_targets)

val_prob=model.predict_proba(X_val)
val_prob

val_targets.value_counts()/len(val_targets)

model.score(X_test,test_targets)

test_prob=model.predict_proba(X_test)
test_prob

from sklearn.tree import plot_tree, export_text

plt.figure(figsize=(80,20))
plot_tree(model,feature_names=X_train.columns,max_depth=3,filled=True);

model.tree_.max_depth

tree_text= export_text(model,max_depth=10,feature_names=list(X_train.columns))
print(tree_text[:5000])

model.feature_importances_

importance_df= pd.DataFrame({
    'feature':X_train.columns,
    'importance':model.feature_importances_
}).sort_values('importance',ascending=False)

importance_df.head(10)

plt.title('Feature Importance')
sns.barplot(data=importance_df.head(10),x='importance',y='feature')

"""NEW WAY OF RANDOM SPLITING

"""

model_1= DecisionTreeClassifier(max_depth=3,random_state=42)

model_1.fit(X_train, train_targets)

model_1.score(X_train, train_targets)

model_1.score(X_val,val_targets)

model_1.score(X_test,test_targets)

model.classes_

plt.figure(figsize=(80,20))
plot_tree(model_1,feature_names=X_train.columns,max_depth=3,filled=True)

print(export_text(model_1, feature_names=list(X_train.columns)))

def max_depth_error(md):
  model=DecisionTreeClassifier(max_depth=md,random_state=42)
  model.fit(X_train,train_targets)
  train_error=1-model.score(X_train,train_targets)
  val_error=1-model.score(X_val,val_targets)
  return {'max_depth': md, 'training_error':train_error,'validation_error':val_error}

error_df= pd.DataFrame([max_depth_error(md) for md in range(1,21)])
error_df

plt.figure()
plt.plot(error_df['max_depth'],error_df['training_error'],label='Training Error')
plt.plot(error_df['max_depth'],error_df['validation_error'],label='Validation Error')
plt.title('Training and Validation Error vs Max Depth')
plt.xticks(range(0,21,2))
plt.xlabel('Max Depth')
plt.ylabel('Error')
plt.legend(['Training','Validation'])

model=DecisionTreeClassifier(max_depth=7,random_state=42).fit(X_train,train_targets)
model.score(X_val,val_targets)

model=DecisionTreeClassifier(max_leaf_nodes=128,random_state=42).fit(X_train,train_targets)
model.score(X_val,val_targets)

model.score(X_train,train_targets)

model.score(X_test,test_targets)

model.tree_.max_depth

model_text=export_text(model,feature_names=list(X_train.columns))
print(model_text[:3000])

def max_error(md,lt):
  model=DecisionTreeClassifier(max_depth=md,max_leaf_nodes=lt,random_state=42)
  model.fit(X_train,train_targets)
  train_error=1-model.score(X_train,train_targets)
  val_error=1-model.score(X_val,val_targets)
  return {'max_depth': md, 'training_error':train_error,'validation_error':val_error}

error_list = []
for md in range(1, 21):
    for lt in range(2, 21):
        error_list.append(max_error(md, lt))


error_df = pd.DataFrame(error_list)
print(error_df)

from sklearn.ensemble import RandomForestClassifier

model=RandomForestClassifier(n_jobs=-1,random_state=42)

model.fit(X_train,train_targets)

model.score(X_train,train_targets)

model.score(X_val,val_targets)

train_prob=model.predict_proba(X_train)
train_prob

len(model.estimators_)

model.estimators_[0]

plt.figure(figsize=(80,20))
plot_tree(model.estimators_[0],max_depth=2,feature_names=X_train.columns,filled=True,rounded=True,class_names=model.classes_)

plt.figure(figsize=(80,20))
plot_tree(model.estimators_[15],max_depth=2,feature_names=X_train.columns,filled=True,rounded=True,class_names=model.classes_)

importance_df=pd.DataFrame({
    'features':X_train.columns,
    'importance':model.feature_importances_,
}).sort_values('importance',ascending=False)

importance_df

plt.title('Feature Importance')
sns.barplot(data=importance_df.head(10),x='importance',y='features')

base_model=RandomForestClassifier(n_jobs=-1,random_state=42).fit(X_train,train_targets)

base_train_acc=base_model.score(X_train,train_targets)
base_train_acc

base_val_acc=base_model.score(X_val,val_targets)
base_val_acc

base_acc=base_train_acc, base_val_acc

model=RandomForestClassifier(n_jobs=-1,random_state=42,n_estimators=10).fit(X_train,train_targets)

model.score(X_train,train_targets)

model.score(X_val,val_targets)

model=RandomForestClassifier(n_jobs=-1,random_state=42,n_estimators=500).fit(X_train,train_targets)

model.score(X_train,train_targets)

model.score(X_val,val_targets)

base_acc

def n_estimators_error(ne):
  model=RandomForestClassifier(n_jobs=-1,random_state=42,n_estimators=ne)
  model.fit(X_train,train_targets)
  train_error=1-model.score(X_train,train_targets)
  val_error=1-model.score(X_val,val_targets)
  return {'estimators': ne, 'training_error':train_error,'validation_error':val_error}

est= pd.DataFrame([n_estimators_error(ne) for ne in range(10,500,50)])
est

plt.figure()
plt.plot(est['estimators'],est['training_error'],label='Training Error')
plt.plot(est['estimators'],est['validation_error'],label='Validation Error')
plt.title('Training and Validation Error vs Max Depth')
plt.xticks(range(10,500,10))
plt.xlabel('Max Depth')
plt.ylabel('Error')
plt.legend(['Training','Validation'])

def test_params(**params):
    model=RandomForestClassifier(n_jobs=-1,random_state=42,**params).fit(X_train,train_targets)
    train_error=1-model.score(X_train,train_targets)
    val_error=1-model.score(X_val,val_targets)
    return model.score(X_train,train_targets),model.score(X_val,val_targets)

test_params(max_depth=5)

test_params(max_depth=26)

test_params(max_leaf_nodes=2**5)

test_params(max_leaf_nodes=2**20)

base_acc

test_params(max_features='log2')

test_params(max_features=3)

test_params(max_features=6)

base_acc

test_params(min_samples_split=3,min_samples_leaf=2)

test_params(min_samples_split=100,min_samples_leaf=60)

base_acc

test_params(min_impurity_decrease=1e-7)

test_params(min_impurity_decrease=1e-2)

base_acc

test_params(bootstrap=False)

base_acc

test_params(max_samples=0.9)

base_acc

model.classes_

test_params(class_weight='balanced')

test_params(class_weight={'No':1,'Yes':2})

base_acc

model=RandomForestClassifier(n_jobs=-1,
                             random_state=42,
                             n_estimators=500,
                             max_features=7,
                             max_depth=30,
                             class_weight={'No':1,'Yes':1.5}
                             )

model.fit(X_train,train_targets)

model.score(X_val,val_targets)

model.score(X_train,train_targets)

base_acc

